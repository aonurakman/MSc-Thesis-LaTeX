%!TEX root = ../main.tex

\chapter{Conclusions and Future Works}
\label{chp:conclusions}

In this chapter, considering everything we discussed up to this point, we will share our vision regarding how can this model be improved and what is left to explore, and then we draw our conclusions. We are hoping to inspire possible further studies on this model and provide a good starting point for them. 

\section{Improvability}

Every decision throughout this study was made by aiming for an efficient utilization of our resources, such as time, computational power, and disk space. These considerations forced us to go with no-so-ideal options from time to time. For instance, in Chapter \ref{chp:experiments}, we repeatedly highlight that the fact that our model can even compete with other pre-trained vectors in some metrics is quite impressive, considering that the data we used for training is significantly smaller. Then instead of training our models using similarly scaled data, we put forward an experiment to show that our models benefit from the larger-sized training data in Section \ref{sec:data_size}. Obviously the former would be more convincing, but we did not have memory capable of handling that big of data, and we did not even have access to a high-performing GPU for the durations mentioned in \cite{w2v2}. Another example is the explorations of hyperparameter configurations space or other design choices, which we did our best given our limited time, but surely there is way more to see than the versions described in Section \ref{sec:variants}. For instance, we could not explore the effects of other values of learning rates and decay rates, or different optimizers, due to the time constraints.

Therefore there is definitely more to explore, and some of these new things to try are most likely will yield better performances. Now we list some of the things that we did not make it in the duration of our study, but can potentially improve the model performance or provide new insights if applied.

\subsection{Change of training data}

The observations in Section \ref{sec:data_size} allowed us to believe that our model can indeed perform better if more training data is provided. Coming from this, we can argue that training our model using a larger training dataset is definitely worth trying. However, we should note day this will require significantly higher memory and perhaps a day-long training time. Using a large corpus, such as a recent version of Wikipedia Dumps or Google News, the vocabulary will be much more extensive and the words will be observed in many different contexts.

\subsection{Data partition}

Our focus in this study was to train a word embedding model on a sufficiently large dataset and observe its characteristics comparatively to the standard models. We also dealt with time and computation limitations. Therefore we were not interested in exploring the smarter ways to partition our data into training and validation sets, nor using a test set to test our training performance. For future studies, where the objective is to produce a set of word embeddings to compete with the state-of-the-art models, one should consider these factors in more depth. For instance, the validation split can be done more mindfully, perhaps using a technique like K-Fold Cross Validation. Also, an additional layer of evaluation can be implemented, using a test partition from the training set. The results of this sort of evaluation would not tell us much about the model's performance on any intrinsic or extrinsic assessments, but it would give the developers a clearer picture of which model learned the most efficiently from the training data, compared to its variants.

\subsection{Dealing with overfitting}
We see clearly in our learning curves that our models overfit the training data, just after a few epochs. This may be caused by several things, such as the choice of the dimensionality of the vector space, given the size of the training data. It is not possible to estimate what would happen if the training data were selected differently, but in case it is again observed in future studies, the designers shall consider how to deal with this issue so that the final product will generalize better. One way to cope with this is a regularization technique called \textit{Early Stopping}, in which the training is finalized earlier than the set number of epochs if the performance on the validation set does not improve or gets worse (based on a predefined \textit{tolerance}) for a predefined number of steps.

\subsection{Learning rate decay}
Learning rate scheduling is an approach that allows us to systematically decay the learning step size as the training iterates. This is employed in order not to diverge from the found solution, and converge to a local minima that otherwise we would not be able to reach with a large step size. Decreasing the step size might make it slower to reach a solution. However, we believe that this technique is highly suitable for our case, given that our model already starts to overfit from very early on. This technique is something we briefly tried in our version 2 (see Section \ref{sec:variants}), and it provided some promising results. However, we believe that our attempt was not sufficient to fully explore the potential benefits.

\subsection{Stopword removal and subsampling}
As we described earlier in Section \ref{sec:preprocessing}, we remove an extensive set of stopwords from the corpus in the preprocessing step. After, we also apply the subsampling operation, removing some occurrences of frequent words, in order to balance the representations. We believe this will enable us to capture more contextual relationships of words, by removing the words that fill up the context windows without carrying much information. However, our way of doing it is rather strict, and this was needed to further improve the training efficiency. Therefore further studies are strongly encouraged to loosen up this procedure to see if preserving some of these words will improve the model performance.

\subsection{Embedding dimensionality}
We chose 300-dimensional vector space to embed our words, as it is a widely used number in the literature. However, given that our training data is significantly smaller than how it is in most of the studies, if we had gone with a small dimension selection for our smaller vocabulary, perhaps we could have prevented overfitting. Similarly, in the case of training our model with a larger corpus, perhaps more dimensions would allow our third-order model to encode more information.

\subsection{Normalization in loss function}
We observe that as the training proceeds, our vectors get longer and longer. We had no reason to believe that this might influence the model performance either negatively or positively. However, further experiments can be conducted to investigate this effect, or better, the vector projections can be normalized using the target vector length to see the change in the model performance.

\subsection{Understanding the model behavior}

As we described in Section \ref{sec:stats} and Section \ref{sec:vis_proj} we spared a significant amount of time for understanding how our models learn word representations as the training proceeds. However, we believe that we still did not fully explain our model's behavior. Understanding our model in depth might seem redundant as long as we produce high-quality embeddings, but we trust that this is vital for making it even better or inspiring further studies and new models. We did not present in this dissertation, but we also attempted to visualize our embeddings in three dimensions using dimensionality reduction techniques. However, these efforts suffered from loss of information, and the final results were not as expressive as we hoped. Finding new visualization techniques, or simply a more in-depth thought process, might provide new material for improving our model or prove its advantages over other standardized models.

\subsection{Downstream NLP tasks}
We decided to work with intrinsic evaluation techniques, which are to assess the quality of word embeddings as they are, with respect to some provided ground truth. This is the most straightforward way to get an idea about the quality of a word embedding model but by no means is it the only way. Alternatively, we could have opted to employ extrinsic methods, which involve evaluating word embedding vectors or comparing two sets of embeddings by integrating them into downstream \ac{NLP} tasks and subsequently evaluating the resultant systems. We cannot argue that this way would provide more reliable results, but this multifaceted approach enables a more comprehensive understanding of the versatility and effectiveness of word embeddings within broader \ac{NLP} contexts. 

We are optimistic that our model is capable of yielding even more impressive results for tasks that demand finer contextual distinctions. However, we should note that our model may not offer the same level of plug-and-play convenience as others. Just like we did in designing the evaluation task "Word Sense Distinction", designers should be aware of the distinct nature of our vectors and design an appropriate method to use these vectors to their full potential.

\newpage
\section{Final remarks}

In this dissertation, we aimed to present our work as a self-contained and comprehensible entity. First, we tried our best to give the reader a theoretical background before proceeding to our work, talking about more generalized concepts like Machine Learning and Natural Language Processing, and then focusing on word embeddings and some of the state-of-the-art models. Later in Chapter \ref{chp:design}, we described our idea and how we mathematically formulate it. Following this blueprint, Chapter \ref{chp:implementation} was about how we programmed our training mechanism. We then evaluated our products and discussed our findings in Chapter \ref{chp:experiments}, providing some numbers and visuals to make things clear. Finally, we provide our ideas for further explorations and remarks in this chapter, hoping to inspire efforts to improve this model. We tried to balance different aspects of this project in our descriptions, leveraging formulas, code blocks, plots, and of course, plain text, to make things as understandable as possible for readers from different backgrounds.

We are glad and excited to present a fresh perspective on a long-standing problem within the field of \ac{NLP}. Given the unique nature of our approach, which lacks direct benchmarks from prior studies, we acknowledge that we needed to be careful while designing evaluation processes and commenting on the results, as it is not easy to validate them. Nevertheless, we tried our best to remain as objective and rigorous as possible, fairly assessing our models and highlighting their weaknesses as much as their strengths.

We spent quite a bit of time brainstorming and literature reviewing, in order to explore what kind of experiments we could conduct to effectively visualize our vectors and how can we assess them. We picked some of the most popular studies in this domain and found their most compatible models, in order to provide a benchmark for our products. We aimed to effectively demonstrate what has now been achieved and what can still be improved.

Our models produce vectors that can capture contextual relationships of words and take a fraction of data to train compared to other standard models. We believe there is still more to explore and interpret about this approach, but we are happy to put forward a new perspective on static word embeddings, which are inherently thought to be rather rigid and limited. Today's latest \ac{NLP} advancements are based on higher-level concepts such as large language models, question-answering systems, or automatic translation. However, static word embeddings still hold relevance in today's NLP advancements by offering efficiency, simplicity, and as we believe, improvability.

As we now conclude our remarks, we would like to emphasize the significance of continuous advancement in the foundational elements of a system, in order to overcome what was previously thought to be a limitation. It is important to find new ways to build the next biggest and the most impressive skyscraper, but innovating how we produce bricks can always unlock entirely new possibilities. Similarly, we believe that the advancements in static word embeddings can offer unthinkable advancements in the higher-level \ac{NLP} tasks. With this in mind, we hope that the work we presented here is useful for enhancing the way we produce static word embeddings today, or inspirational for those who are looking for new ways to produce them.



